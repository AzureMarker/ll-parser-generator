use logos::{Lexer, Logos};
use std::ops::Range;

#[derive(Logos, Debug, Clone)]
pub enum Token<'input> {
    #[token("grammar")]
    Grammar,
    #[token("token")]
    Token,
    #[token("use")]
    Use,
    #[token(";")]
    Semicolon,
    #[token(":")]
    Colon,
    #[token("{")]
    LBrace,
    #[token("}")]
    RBrace,
    #[token("=>")]
    EqArrow,
    #[token("=")]
    Equal,
    #[token(",")]
    Comma,
    #[token(")")]
    RParen,
    #[token("(")]
    LParen,
    #[token("[")]
    LBracket,
    #[token("]")]
    RBracket,
    #[token("|")]
    VBar,

    #[regex("[a-zA-Z][a-zA-Z0-9]*")]
    Identifier(&'input str),

    // These two tokens are generated by the sub-lexers ImportToken and ActionToken
    ImportCode(&'input str),
    ActionCode(&'input str),

    #[error]
    // Skip whitespace
    #[regex(r"[ \t\n\f]+", logos::skip)]
    Error,
}

#[derive(Logos)]
enum ImportToken<'input> {
    // FIXME: This token represents a Rust import expression (everything after
    //        "use" and before ";").
    #[token("import_FIXME")]
    ImportCode(&'input str),

    #[error]
    // Skip whitespace
    #[regex(r"[ \t\n\f]+", logos::skip)]
    Error,
}

#[derive(Logos)]
enum ActionToken<'input> {
    // FIXME: This token represents Rust action code (code that appears after
    //        "=>"), which will be lexed by matching parens/braces/brackets.
    #[token("action_FIXME")]
    ActionCode(&'input str),

    #[error]
    // Skip whitespace
    #[regex(r"[ \t\n\f]+", logos::skip)]
    Error,
}

/// Wrap the lexer with some state so it can switch between lexer
/// implementations depending on the context. This is used so we can avoid
/// parsing Rust code, skipping over it in imports and action code blocks.
pub struct StatefulLexer<'input> {
    state: LexerState<'input>,
}

enum LexerState<'input> {
    Normal(Lexer<'input, Token<'input>>),
    Import(Lexer<'input, ImportToken<'input>>),
    ActionCode(Lexer<'input, ActionToken<'input>>),
    Done,
}

impl<'input> StatefulLexer<'input> {
    pub fn new(lexer: Lexer<'input, Token<'input>>) -> Self {
        Self {
            state: LexerState::Normal(lexer),
        }
    }
}

impl<'input> Iterator for StatefulLexer<'input> {
    type Item = Result<(usize, Token<'input>, usize), Range<usize>>;

    fn next(&mut self) -> Option<Self::Item> {
        match std::mem::replace(&mut self.state, LexerState::Done) {
            LexerState::Normal(mut lexer) => {
                let token = lexer.next()?;
                let span = lexer.span();

                self.state = match token {
                    Token::Use => LexerState::Import(lexer.morph()),
                    Token::EqArrow => LexerState::ActionCode(lexer.morph()),
                    _ => LexerState::Normal(lexer),
                };

                Some(match token {
                    Token::Error => Err(span),
                    _ => Ok((span.start, token, span.end)),
                })
            }
            LexerState::Import(mut lexer) => {
                let token = lexer.next()?;
                let span = lexer.span();
                self.state = LexerState::Normal(lexer.morph());
                Some(match token {
                    ImportToken::ImportCode(code) => {
                        Ok((span.start, Token::ImportCode(code), span.end))
                    }
                    ImportToken::Error => Err(span),
                })
            }
            LexerState::ActionCode(mut lexer) => {
                let token = lexer.next()?;
                let span = lexer.span();
                self.state = LexerState::Normal(lexer.morph());
                Some(match token {
                    ActionToken::ActionCode(code) => {
                        Ok((span.start, Token::ActionCode(code), span.end))
                    }
                    ActionToken::Error => Err(span),
                })
            }
            LexerState::Done => None,
        }
    }
}
